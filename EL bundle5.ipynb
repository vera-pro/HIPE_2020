{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage: \n",
    "* Run on the same node as Elasticsearch (for now)\n",
    "* **Specify input and output paths in the config dictionary below**\n",
    "* Relax and hope for the best – it's terribly slow (I used parallel notebooks to get the solution ready on time; a smarter way to speed it up will be used in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'DATA_FILE': 'data_HIPE/training-v1.1/en/HIPE-data-v1.1-dev-en.tsv',\n",
    "    'OUTPUT_FILE': 'submissions/UvA.ILPS_bundle5_EN_DEV.tsv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE=config['DATA_FILE']\n",
    "OUTPUT_FILE=config['OUTPUT_FILE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.0: prepare the data\n",
    "\n",
    "The input data should be in CLEF HIPE format and contain entity mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import read_data_to_dfs_sentences, merge_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77df764988964c938ef6a887b308a74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33174), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dfs = read_data_to_dfs_sentences(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = merge_dfs(raw_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1: prepare the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import StackedEmbeddings, FlairEmbeddings, TransformerWordEmbeddings\n",
    "\n",
    "en_embeddings = StackedEmbeddings([FlairEmbeddings(\"en-impresso-hipe-v1-forward\"), \n",
    "                                   FlairEmbeddings(\"en-impresso-hipe-v1-backward\"),\n",
    "                                   TransformerWordEmbeddings(\"bert-large-cased\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence, segtok_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "\n",
    "# document embedding is an LSTM over stacked word embeddings\n",
    "lstm_embeddings_en = DocumentRNNEmbeddings([en_embeddings], rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.2: prepare Elasticsearch for candidate extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ilps-cn007',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'vU7MO42lR2KzA6bPvSP_pA',\n",
       " 'version': {'number': '7.7.0',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'tar',\n",
       "  'build_hash': '81a1e9eda8e6183f5237786246f6dced26a10eaf',\n",
       "  'build_date': '2020-05-12T02:01:37.602180Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.5.1',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: searching for candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.elastic_getters import wikidata_search_, wikidata_search_precise, wikidata_search_fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_variants(cur_sequence, cur_len, target_len, remaining_variants_per_word):\n",
    "    '''\n",
    "    For entities longer than one word - combines all possible variants of historical spelling per word.\n",
    "    Returns a list of strings, each representing one option of spelling the entire entity\n",
    "    \n",
    "    Params:\n",
    "        cur_sequence: a string containing currently obtained spelling options\n",
    "        cur_len: length of cur_sequence, i.e. number of words processed\n",
    "        target_len: target number of words (length of the entity)\n",
    "        remaining_variants_per_word: list of lists, each containing spelling variants of a word not yet processed\n",
    "    '''\n",
    "    if cur_len == target_len:\n",
    "        return [cur_sequence.strip()]\n",
    "    \n",
    "    cur_variants = remaining_variants_per_word[0]\n",
    "    res = []\n",
    "    for variant in cur_variants:\n",
    "        res.extend(combine_variants(cur_sequence+' '+variant, cur_len+1, target_len,\n",
    "                                   remaining_variants_per_word[1:]))\n",
    "    return res\n",
    "    \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natas\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def gen_word_variants(entity):\n",
    "    '''\n",
    "    Returns a list of entity spelling variants, obtained using the natas library\n",
    "    and combined using the function above\n",
    "    '''\n",
    "    words = [w.strip() for w in entity.split(' ') if w]\n",
    "    raw_variants_per_word = natas.normalize_words(words, n_best=3)\n",
    "    \n",
    "    variants_per_word = []\n",
    "    for i, item in enumerate(raw_variants_per_word):\n",
    "        cur_variants = item if item else [words[i]]\n",
    "        if words[i] not in cur_variants:\n",
    "            cur_variants.append(words[i])\n",
    "        variants_per_word.append(cur_variants)\n",
    "        \n",
    "    assert len(variants_per_word) == len(words)\n",
    "    \n",
    "    return combine_variants('', 0, len(words), variants_per_word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(es, entity):\n",
    "    '''\n",
    "    Searches for an entity in Elasticsearch.\n",
    "    Is very much suboptimal and will be updated in the future – now left as it is for reproducibility\n",
    "    '''\n",
    "    \n",
    "    hits = wikidata_search_(es, entity) \n",
    "    hits_exact = wikidata_search_precise(es, entity)\n",
    "    hits_fuzzy = wikidata_search_fuzzy(es, entity)\n",
    "    \n",
    "    hits_variants = [] # searching for historical spelling variants\n",
    "    if len(entity.split(' ')) < 4: # only for short entities, otherwise it will take forever\n",
    "        variants = gen_word_variants(entity)\n",
    "        for variant in variants:\n",
    "            hits_variants.extend(wikidata_search_precise(es, variant))\n",
    "        \n",
    "    \n",
    "    if not hits_exact and len(entity.split(' ')) > 4: # a terribly long entity - try searching for its first 2 words\n",
    "        shorter_entity = \" \".join(entity.split(' ')[:2])\n",
    "        variants = gen_word_variants(shorter_entity)\n",
    "        for variant in variants:\n",
    "            hits_variants.extend(wikidata_search_fuzzy(es, variant))\n",
    "        \n",
    "    res_raw = hits_exact + hits + hits_fuzzy + hits_variants\n",
    "    \n",
    "    # removing duplicates:\n",
    "    res_pure = []\n",
    "    seen_Qs = set()\n",
    "    for entry in res_raw:\n",
    "        Q = entry['_source']['label_exact']\n",
    "        if Q not in seen_Qs:\n",
    "            seen_Qs.add(Q)\n",
    "            res_pure.append(entry)\n",
    "            \n",
    "    # removing least relevant results:\n",
    "    res_filtered = []\n",
    "    if not res_pure:\n",
    "        return res_pure # if nothing at all is found (happens very rarely)\n",
    "    \n",
    "    best_score = res_pure[0]['_score'] if res_pure[0]['_score'] else 20\n",
    "    for item in res_pure:\n",
    "        if item['_score'] and item['_score'] > 0.6 * best_score:\n",
    "            res_filtered.append(item)\n",
    "        if not item['_score']: # it means the results were sorted already - just do nothing\n",
    "            res_filtered.append(item)\n",
    "            \n",
    "    return res_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment the cell below to test candidate search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'eRo5LXIByThoYigYu24R',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q45',\n",
       "   'label': 'portugal',\n",
       "   'count': 140530,\n",
       "   'id': 36166617,\n",
       "   'label_exact': 'Q45'},\n",
       "  'sort': [140530]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'B7XRLHIByThoYigY1BFA',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q14110517',\n",
       "   'label': 'portugal',\n",
       "   'count': 1161,\n",
       "   'id': 4043769,\n",
       "   'label_exact': 'Q14110517'},\n",
       "  'sort': [1161]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'KIqyLXIByThoYigYwOhW',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q7232531',\n",
       "   'label': 'portugal',\n",
       "   'count': 267,\n",
       "   'id': 64240553,\n",
       "   'label_exact': 'Q7232531'},\n",
       "  'sort': [267]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'I-glLXIByThoYigY4VSE',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q415558',\n",
       "   'label': 'portugal',\n",
       "   'count': 204,\n",
       "   'id': 32533667,\n",
       "   'label_exact': 'Q415558'},\n",
       "  'sort': [204]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'E57OLHIByThoYigYScRk',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q10638037',\n",
       "   'label': 'portugal',\n",
       "   'count': 180,\n",
       "   'id': 640319,\n",
       "   'label_exact': 'Q10638037'},\n",
       "  'sort': [180]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 's7HRLHIByThoYigYOzd-',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q13498039',\n",
       "   'label': 'portugal',\n",
       "   'count': 96,\n",
       "   'id': 3399390,\n",
       "   'label_exact': 'Q13498039'},\n",
       "  'sort': [96]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'BjWVLXIByThoYigY1Kvp',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q66987006',\n",
       "   'label': 'portugal',\n",
       "   'count': 41,\n",
       "   'id': 58475703,\n",
       "   'label_exact': 'Q66987006'},\n",
       "  'sort': [41]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'NndbLXIByThoYigYGOFv',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q51520273',\n",
       "   'label': 'portugal',\n",
       "   'count': 39,\n",
       "   'id': 43040726,\n",
       "   'label_exact': 'Q51520273'},\n",
       "  'sort': [39]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'QF_zLHIByThoYigY6MJD',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q31697898',\n",
       "   'label': 'portugal',\n",
       "   'count': 36,\n",
       "   'id': 21939535,\n",
       "   'label_exact': 'Q31697898'},\n",
       "  'sort': [36]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'OwDeLHIByThoYigYiPBW',\n",
       "  '_score': None,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q22323850',\n",
       "   'label': 'portugal',\n",
       "   'count': 34,\n",
       "   'id': 12414606,\n",
       "   'label_exact': 'Q22323850'},\n",
       "  'sort': [34]},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'fzpFLXIByThoYigYVuKJ',\n",
       "  '_score': 13.817722,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q47124618',\n",
       "   'label': 'daniel portugal portugal',\n",
       "   'count': 29,\n",
       "   'id': 38370167,\n",
       "   'label_exact': 'Q47124618'}},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'aEXtLHIByThoYigYceAk',\n",
       "  '_score': 13.219944,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q29016795',\n",
       "   'label': 'portal portugal portugal lists',\n",
       "   'count': 17,\n",
       "   'id': 19218993,\n",
       "   'label_exact': 'Q29016795'}},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'GUXtLHIByThoYigYceAU',\n",
       "  '_score': 13.219944,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q29016664',\n",
       "   'label': 'portal portugal portugal news',\n",
       "   'count': 17,\n",
       "   'id': 19218884,\n",
       "   'label_exact': 'Q29016664'}},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'G_PcLHIByThoYigYFI_X',\n",
       "  '_score': 12.943723,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q21087356',\n",
       "   'label': 'portugal',\n",
       "   'count': 28,\n",
       "   'id': 11099928,\n",
       "   'label_exact': 'Q21087356'}},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'qFnyLHIByThoYigYtZtU',\n",
       "  '_score': 12.943723,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q31103884',\n",
       "   'label': 'portugal ',\n",
       "   'count': 83,\n",
       "   'id': 21403180,\n",
       "   'label_exact': 'Q31103884'}},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'HopiLXIByThoYigYANZo',\n",
       "  '_score': 12.943723,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q52809490',\n",
       "   'label': 'portugal ',\n",
       "   'count': 95,\n",
       "   'id': 44339654,\n",
       "   'label_exact': 'Q52809490'}},\n",
       " {'_index': 'wikidata_clef',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'WIgBLXIByThoYigYw7Zf',\n",
       "  '_score': 12.943723,\n",
       "  '_source': {'uri': 'http://www.wikidata.org/entity/Q35267157',\n",
       "   'label': 'portugal',\n",
       "   'count': 25,\n",
       "   'id': 25737496,\n",
       "   'label_exact': 'Q35267157'}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_candidates(es, 'Portugal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ranking the candidates using their wikidata descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.elastic_getters import wikidata_get_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_description(es, Q='Q14110517', lang='en'):\n",
    "    '''\n",
    "    Returns an entity description from wikidata\n",
    "    '''\n",
    "    res = wikidata_get_description(es, Q)\n",
    "    if res:\n",
    "        return res[0]['_source']['description']\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removing punctuation\n",
    "'''\n",
    "\n",
    "import string\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "\n",
    "def remove_punctuation(s):\n",
    "    new_s = s.translate(table)\n",
    "    #also take care of extra whitespaces if they happen\n",
    "    new_s = ' '.join(new_s.split()).strip(' ')\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Replacing punctuation with whitespaces - same as above basically\n",
    "'''\n",
    "\n",
    "translator = str.maketrans(string.punctuation + '’', ' '*(len(string.punctuation)+1))\n",
    "def replace_punctuation_with_spaces(s):\n",
    "    new_s = s.translate(translator)\n",
    "    #also take care of extra whitespaces if they happen\n",
    "    new_s = ' '.join(new_s.split()).strip(' ')\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculating Levenstein similarity between strings\n",
    "'''\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def lev_similarity(s1, s2):\n",
    "    ratio = fuzz.ratio(s1.lower(), s2.lower())\n",
    "    return ratio/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import CosineSimilarity\n",
    "cos = CosineSimilarity(dim=1)\n",
    "\n",
    "THRESHOLD = 0.7\n",
    "def get_similarities_replacing(sentence, entity, start_pos, es, embeddings=lstm_embeddings_en, \n",
    "                               window_size=5, candidate_window_size=5, lang='en'): \n",
    "    \n",
    "    '''\n",
    "    Takes a sentence and an entity mention inside it, returns a ranked list of candidates\n",
    "    \n",
    "        sentence: a sentence containing the entity mention; string\n",
    "        entity: the mentioned entity; string\n",
    "        start_pos: number of the first word of the entity in the sentence\n",
    "        \n",
    "        es: ElasticSearch\n",
    "        embeddings: contextualised document embeddings from Flair\n",
    "        window_size: number of words to be used for symmetrical context surrounding the entity within the sentence\n",
    "         (window_size=5 -> include 5 words to the left and 5 to the right)\n",
    "         \n",
    "        candidate_window_size: maximum length of the candidate description to be inserted in the sentence\n",
    "        lang: now only English is supported, but I hope to try it with French and German later\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Embedding the part of the sentence surrounding the entity:\n",
    "    sent = remove_punctuation(sentence).split(' ')\n",
    "    entity = replace_punctuation_with_spaces(entity)\n",
    "    \n",
    "    end_pos = start_pos + len(entity.split(' '))\n",
    "    \n",
    "    target_left = sent[max(0, start_pos-window_size):start_pos] # left part of the sentence\n",
    "    target_right = sent[end_pos:min(len(sent), end_pos+window_size)]\n",
    "    \n",
    "    target_context = target_left + \\\n",
    "                        sent[start_pos:end_pos] + \\\n",
    "                        target_right\n",
    "    \n",
    "    target = Sentence(\" \".join(target_context))\n",
    "    embeddings.embed(target)\n",
    "    target_vector = target.embedding.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    # Substituting candidate descriptions and calculating their scores:\n",
    "    res = []\n",
    "    candidates = get_candidates(es, entity)\n",
    "    for candidate in candidates:\n",
    "        Q = candidate['_source']['label_exact']\n",
    "        label = candidate['_source']['label']\n",
    "        desc_raw = get_description(es, Q, lang)\n",
    "        if not desc_raw:\n",
    "            desc_raw = label\n",
    "            \n",
    "        desc = remove_punctuation(desc_raw).split(' ')\n",
    "\n",
    "        # Getting a new vector embedding:\n",
    "\n",
    "        desc_context = desc[0:min(candidate_window_size, len(desc))]\n",
    "\n",
    "        replaced = \" \".join(target_left + desc_context + target_right)\n",
    "\n",
    "        seq = Sentence(replaced)\n",
    "\n",
    "        embeddings.embed(seq)\n",
    "        candidate_vector = seq.embedding.unsqueeze(0)\n",
    "\n",
    "        # Measuring distance:\n",
    "        \n",
    "        distance_context = cos(target_vector, candidate_vector).item()\n",
    "        \n",
    "        w_short = 0.53+math.sqrt(1/math.log(100+len(Q))) # a heuristic to ever-so-slightly prefer shorter Q labels\n",
    "        w_lev = lev_similarity(label, entity) # use Levenstein distance ratio as a weight when calculating the scores\n",
    "        distance = distance_context * w_lev * w_short\n",
    "        \n",
    "        res.append((Q, distance, label + ' ' + \" \".join(desc_context)))\n",
    "     \n",
    "    # Sorting the results by score\n",
    "    res = sorted(list(set(res)), key = lambda x: -x[1])\n",
    "    \n",
    "    # Now, adding NIL:\n",
    "    res_with_nil = []\n",
    "    nil_added = False\n",
    "    for q, score, desc in res:\n",
    "        if score < THRESHOLD and not nil_added:\n",
    "            res_with_nil.append(('NIL', THRESHOLD, \"\"))\n",
    "            nil_added = True\n",
    "            \n",
    "        res_with_nil.append((q, score, desc))\n",
    "        \n",
    "    return res_with_nil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment the cell below to test candidate search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Q21', 0.920724974151172, 'england country in northwest Europe part'),\n",
       " ('Q20398466', 0.8923825737585285, 'england painting by Richard Ansdell'),\n",
       " ('Q17629554', 0.891240048642801, 'england album'),\n",
       " ('Q79282', 0.8849781282991566, 'england city in Lonoke County Arkansas'),\n",
       " ('Q27881912', 0.8774320394566534, 'england painting by Thomas Creswick'),\n",
       " ('Q2131751', 0.8757769562449264, 'england British progressive rock band'),\n",
       " ('Q257294', 0.8716176641365393, 'england Wikimedia disambiguation page'),\n",
       " ('Q11111401', 0.8612603645698638, 'england family name'),\n",
       " ('Q17653976', 0.836010612710725, 'england Wikinews article'),\n",
       " ('Q62061800', 0.825158486056637, 'england manuscript map drawn by Erwin'),\n",
       " ('NIL', 0.7, ''),\n",
       " ('Q138046', 0.5779662168727834, 'england  england novel'),\n",
       " ('Q60410910', 0.4829642873290349, ' england  england  1998 edition'),\n",
       " ('Q5377954',\n",
       "  0.46817721307320653,\n",
       "  'england  my england 1995 film by Tony Palmer'),\n",
       " ('Q5377956',\n",
       "  0.4199796295839431,\n",
       "  'england  their england book by A G Macdonell')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_similarities_replacing(\"We went to England for a business trip\", \"England\", 3, es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: performing entity linking for all sentences in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import df_to_sentence, extract_entity_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfba0f8cdec4f8dbb53c6eace057b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1084), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs_with_links = []\n",
    "for i, df in enumerate(tqdm(dfs)):\n",
    "    # Handling word wrapping and NoSpaceAfter flags while preserving the data format:\n",
    "    sentence = df_to_sentence(df)\n",
    "    mentions = extract_entity_mentions(df)\n",
    "    new_tokens = sentence.split(' ')\n",
    "    \n",
    "    # Entity linking starts:\n",
    "    df_with_links = df.copy()\n",
    "    for pos, raw_label in mentions:\n",
    "        num_words = len(raw_label.split(\" \"))\n",
    "        label = df_to_sentence(df[pos:pos+num_words]) # in case the label has word wrapping too\n",
    "        \n",
    "        num_removed_spaces = sum(1 for item in df['MISC'].tolist()[:pos] if item == 'NoSpaceAfter')\n",
    "        pos_in_sentence = pos-num_removed_spaces\n",
    "        \n",
    "        res = get_similarities_replacing(sentence, label, pos_in_sentence, es)\n",
    "        \n",
    "        found_qs = [item[0] for item in res]\n",
    "        \n",
    "        answer = \"|\".join(found_qs[:min(5, len(found_qs))])\n",
    "        \n",
    "        # Adding the links\n",
    "        cur_pos = pos\n",
    "        while cur_pos - pos < num_words:\n",
    "            df_with_links['NEL-LIT'][cur_pos] = answer\n",
    "            df_with_links['NEL-METO'][cur_pos] = answer\n",
    "            cur_pos += 1\n",
    "            \n",
    "    dfs_with_links.append(df_with_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>NE-COARSE-LIT</th>\n",
       "      <th>NE-COARSE-METO</th>\n",
       "      <th>NE-FINE-LIT</th>\n",
       "      <th>NE-FINE-METO</th>\n",
       "      <th>NE-FINE-COMP</th>\n",
       "      <th>NE-NESTED</th>\n",
       "      <th>NEL-LIT</th>\n",
       "      <th>NEL-METO</th>\n",
       "      <th>MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TOKEN, NE-COARSE-LIT, NE-COARSE-METO, NE-FINE-LIT, NE-FINE-METO, NE-FINE-COMP, NE-NESTED, NEL-LIT, NEL-METO, MISC]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_with_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import write_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(dfs_with_links, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
